# H∆∞·ªõng D·∫´n X·ª≠ L√Ω D·ªØ Li·ªáu ElonMusk_tweets.csv

_(S·ª≠ d·ª•ng Hadoop MapReduce & Apache Spark v·ªõi tr·ª±c quan h√≥a d·ªØ li·ªáu)_

File n√†y h∆∞·ªõng d·∫´n c√°c b∆∞·ªõc sau:

- T·∫°o c√°c file Python (mapper, reducer) b·∫±ng nano.
- Upload file d·ªØ li·ªáu l√™n HDFS.
- Ch·∫°y job MapReduce ƒë·∫øm s·ªë tweet theo ng√†y v√† theo khung gi·ªù.
- X·ª≠ l√Ω d·ªØ li·ªáu b·∫±ng Apache Spark v√† tr·ª±c quan h√≥a k·∫øt qu·∫£ b·∫±ng bi·ªÉu ƒë·ªì.

> **Ghi ch√∫:** C√°c b∆∞·ªõc c√†i ƒë·∫∑t Java, Hadoop, Spark ƒë√£ ƒë∆∞·ª£c th·ª±c hi·ªán.  
> T√†i kho·∫£n HDFS ƒë∆∞·ª£c s·ª≠ d·ª•ng: **hdoop**.

---

## Ph·∫ßn I: Hadoop MapReduce

### 1. T·∫°o c√°c file Python cho MapReduce

#### 1.1 T·∫°o file `mapper_date.py`

M·ªü terminal v√† nh·∫≠p:

```bash
nano mapper_date.py
```

Sau ƒë√≥, d√°n n·ªôi dung sau:

```bash
import sys

for line in sys.stdin:
    line = line.strip()
    if not line:
        continue
    fields = line.split()
    # D·ªØ li·ªáu c√≥ √≠t nh·∫•t 4 tr∆∞·ªùng: id, date, time, text
    if len(fields) < 4:
        continue
    date = fields[1]  # Tr∆∞·ªùng th·ª© 2 l√† ng√†y
    print(f"{date}\t1")
```

L∆∞u file b·∫±ng c√°ch nh·∫•n Ctrl+X, sau ƒë√≥ Y v√† Enter.

1.2 T·∫°o file reducer_date.py
M·ªü terminal:

```bash
nano reducer_date.py
```

D√°n n·ªôi dung sau:

```bash
import sys

current_date = None
count = 0

for line in sys.stdin:
    line = line.strip()
    if not line:
        continue
    date, value = line.split("\t")
    value = int(value)

    if current_date == date:
        count += value
    else:
        if current_date is not None:
            print(f"{current_date}\t{count}")
        current_date = date
        count = value

if current_date is not None:
    print(f"{current_date}\t{count}")
```

L∆∞u file.

1.3 T·∫°o file mapper_hour.py
M·ªü terminal:

```bash
nano mapper_hour.py
```

D√°n n·ªôi dung sau:

```bash
import sys

for line in sys.stdin:
line = line.strip()
if not line:
continue
fields = line.split()
if len(fields) < 4:
continue
time_field = fields[2] # Tr∆∞·ªùng th·ª© 3 l√† th·ªùi gian (HH:MM:SS)
hour = time_field.split(":")[0]
print(f"{hour}\t1")
```

L∆∞u file.

1.4 T·∫°o file reducer_hour.py
M·ªü terminal:

```bash
nano reducer_hour.py
```

D√°n n·ªôi dung sau:

```bash
import sys

current_hour = None
count = 0

for line in sys.stdin:
line = line.strip()
if not line:
continue
hour, value = line.split("\t")
value = int(value)

    if current_hour == hour:
        count += value
    else:
        if current_hour is not None:
            print(f"{current_hour}:00 - {current_hour}:59\t{count}")
        current_hour = hour
        count = value

if current_hour is not None:
print(f"{current_hour}:00 - {current_hour}:59\t{count}")
```

L∆∞u file.

1.5 C·∫•p quy·ªÅn th·ª±c thi cho c√°c file
Trong terminal, ch·∫°y l·ªánh:

```bash
chmod +x mapper_date.py reducer_date.py mapper_hour.py reducer_hour.py
```

2. Upload d·ªØ li·ªáu l√™n HDFS

   Kh·ªüi ƒë·ªông hdoop

   ```bash
   su - hdoop
   ```

   ```bash
   start-all.sh
   ```

   Gi·∫£ s·ª≠ file ElonMusk_tweets.csv n·∫±m t·∫°i /home/phat/Downloads/ElonMusk_tweets.csv. Upload file l√™n HDFS v·ªõi t√†i kho·∫£n hdoop b·∫±ng l·ªánh:

```bash
hdfs dfs -mkdir -p /user/hdoop/data
hdfs dfs -copyFromLocal /home/phat/Downloads/ElonMusk_tweets.csv /user/hdoop/data
```

3. Ch·∫°y c√°c job MapReduce

3.1 ƒê·∫øm s·ªë tweet theo ng√†y
Ch·∫°y job MapReduce v·ªõi l·ªánh:

```bash
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-\*.jar \
 -input /user/hdoop/data/ElonMusk_tweets.csv \
 -output /user/hdoop/data/tweet_count_by_date \
 -mapper mapper_date.py \
 -reducer reducer_date.py
```

Sau khi job ho√†n t·∫•t, xem k·∫øt qu·∫£:

```bash
hdfs dfs -cat /user/hdoop/data/tweet_count_by_date/part-\*
```

3.2 ƒê·∫øm s·ªë tweet theo khung gi·ªù
Ch·∫°y job MapReduce v·ªõi l·ªánh:

```bash
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-\*.jar \
 -input /user/hdoop/data/ElonMusk_tweets.csv \
 -output /user/hdoop/data/tweet_count_by_hour \
 -mapper mapper_hour.py \
 -reducer reducer_hour.py
```

Xem k·∫øt qu·∫£:

```bash
hdfs dfs -cat /user/hdoop/data/tweet_count_by_hour/part-\*
```

```bash
stop-all.sh
```

Ph·∫ßn II: Apache Spark v√† Tr·ª±c Quan H√≥a D·ªØ Li·ªáu

1. T·∫°o file Spark Job v·ªõi Bi·ªÉu ƒê·ªì: tweet_analysis.py
   M·ªü terminal:

```bash
nano tweet_analysis.py
```

D√°n n·ªôi dung sau v√†o file (ch·ªânh s·ª≠a ƒë∆∞·ªùng d·∫´n file n·∫øu c·∫ßn: n·∫øu d·ªØ li·ªáu ƒëang ·ªü local ho·∫∑c tr√™n HDFS):

```bash
from pyspark.sql import SparkSession

# Kh·ªüi t·∫°o SparkSession
spark = SparkSession.builder.appName("TweetAnalysis").getOrCreate()
sc = spark.sparkContext

# ƒê·ªçc file d·ªØ li·ªáu
rdd = sc.textFile("file:///home/phat/Downloads/ElonMusk_tweets.csv")

# (a) ƒê·∫øm s·ªë tweet theo ng√†y
def extract_date(line):
    fields = line.split()
    if len(fields) < 4:
        return None
    date = fields[1]
    return (date, 1)

tweet_by_date = rdd.map(extract_date).filter(lambda x: x is not None).reduceByKey(lambda a, b: a + b)
tweet_by_date_sorted = tweet_by_date.sortByKey()
tweet_by_date_str = tweet_by_date_sorted.map(lambda x: f"{x[0]},{x[1]}")
tweet_by_date_str.coalesce(1).saveAsTextFile("tweet_count_by_date.txt")

# (b) ƒê·∫øm s·ªë tweet theo khung gi·ªù
def extract_hour(line):
    fields = line.split()
    if len(fields) < 4:
        return None
    time_field = fields[2]
    hour = time_field.split(":")[0]
    return (hour, 1)

tweet_by_hour = rdd.map(extract_hour).filter(lambda x: x is not None).reduceByKey(lambda a, b: a + b)
tweet_by_hour_sorted = tweet_by_hour.sortByKey()
tweet_by_hour_str = tweet_by_hour_sorted.map(lambda x: f"{x[0]},{x[1]}")
tweet_by_hour_str.coalesce(1).saveAsTextFile("tweet_count_by_hour.txt")

# üìå T√¨m khung gi·ªù Elon Musk hay tweet nh·∫•t
most_active_hour = tweet_by_hour.max(lambda x: x[1])  # T√¨m gi·ªù c√≥ nhi·ªÅu tweet nh·∫•t
most_active_hour_str = f"Elon Musk th∆∞·ªùng ƒëƒÉng tweet v√†o khung gi·ªù: {most_active_hour[0]} v·ªõi {most_active_hour[1]} tweet.\n"

# Ghi k·∫øt qu·∫£ ra file
with open("most_active_hour.txt", "w") as f:
    f.write(most_active_hour_str)

print(most_active_hour_str)  # In ra terminal

spark.stop()


```

L∆∞u file (Ctrl+X, Y, Enter).

2. Ch·∫°y Spark Job

Trong terminal, ch·∫°y l·ªánh:

```bash
spark-submit tweet_analysis.py
```

K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c tr·ª±c quan h√≥a b·∫±ng bi·ªÉu ƒë·ªì v√† c√°c file ·∫£nh tweet_count_by_date.png v√† tweet_count_by_hour.png s·∫Ω ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c l√†m vi·ªác.
